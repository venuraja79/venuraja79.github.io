---
layout: single
title:  "Training Language Models for out-of-domain scenarios"
date:   2022-12-20 17:15:20 +0530
categories: jekyll update
---

## Overview
Language Models are the talk of the town. They produce state-of-the-art results in many tasks such as 
semantic search, text classification and summarization when compared with traditional NLP methods. 
However, when presented with same language tasks in a new domain (Engineering, Legal or Biomedical), 
they often come up short and perform less optimal than even traditional techniques. 

We are using a set of techniques to nudge the models to work well in the out of domain scenarios. All the out of domain
scenarios are not same! In some cases, the models can learn the nuances even with a few training examples. In others,
the language model itself have to be augmented. In this paper, we will discuss the techniques and how we leveraged them
in some of our projects to improve the model's performance.

## Language Models
Language models (BERT, GPT) just predict the next word given an initial prompt. They have been trained on large corpus of text 
and have gained intuitive understanding of the language structure, semantics and domain. We almost always download the 
pretrained model checkpoints from [**Hugging Face Hub**](https://huggingface.co) or other open hubs that host the models. 
We can't use these models as is and have to custom-train them. For example, a classification task
requires an architecture change (adding a classification head) and training with a classification dataset. This process is 
called fine-tuning.

## Customizing Language Models
Customizing a model can be as simple as training with a few examples on a particular language task. Sometimes, 
it can be a complex process that involves fully retraining the model with large corpus of domain specific text 
and augmenting its dictionary. We have experience in the following model customizations and this varies for different
problem statements.

* Fine-tuning on a custom task
* Augment the language model for a new domain
* Semantic search in domain specific data
* Aligning the models towards human preferences with Reinforcement Learning

### 1. Task based fine-tuning
Many of the projects that we execute involve fine-tuning. We start with a task specific model checkpoint from the hub and
create a baseline on the task specific performance metric (accuracy, F1-score etc.,). For a Question Answering (QA) system, 
we used the SQUAD trained model as our baseline. This model generalizes well, if the customer's dataset is not very domain specific. 
We typically create a pipeline to generate annotated examples (question-answer pairs from user feedback) 
and fine-tune the model with this additional examples.

For text classification and NER, we start with a Language Model (LM) and add a classification head to it before fine-tuning with a
customer specific dataset as the labels are different for every problem. In the projects that we have executed, the time 
spent on collecting good training data provides the most bang for the buck than the architecture choices and hyperparameter tuning.

Some important points to consider -

* In many cases, a few hundred examples for each class can produce good enough models. When there is a change in architecture,
(addition of classification head) under-fitting is a real concern if the data sizes are small.

* Consider using smaller variants of the models such as DistillBERT to reduce both training and inference times without
losing too much accuracy.

* If the domain is entirely different, more training examples are required. We may also need to pretrain the LM itself, which is
explained in the next section.

* The Attention based transformer language models have a 512 token limitation. In most of the real-life projects, the documents
are large and doesn't fit this 512 token limit. We follow a combination of summarization & pooling methods to overcome this.

* Setup the MLOps process so that further training can be automated.

More details on the [training procedure](https://huggingface.co/docs/transformers/training) is here.

### 2. Augment a Language model for a domain

When the problem domain is different, we first continue to pre-train the language model with domain specific text corpus. 
The augmented model is then fine-tuned as usual for specific tasks such as classification. This has produced better 
results ( a few % points gain in performance) for us in both legal and engineering domain problems that we have attempted.

Every generic pretrained LM comes with its own dictionary which is generated by a sub-word tokenization technique. The first 
step in the customization process is to preprocess the domain corpus to generate a domain specific token list (dictionary).
Then the embeddings for the new tokens are learnt by a training procedure using the domain text.

Please refer the [**ULMFit paper**](https://arxiv.org/pdf/1801.06146.pdf) and the [**notebook**](https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb) for more information.

It is also possible to create a domain specific LM from scratch without actually using any pretrained LM at all. But, this
process requires large text corpus and more GPU compute. This [**paper**](https://arxiv.org/abs/2007.15779) has more details
on this.

### 3. Improving Semantic search results

Most of the search platforms such as elasticsearch/Solr are powered by keyword based algorithms such as BM25 by default. 
Semantic search improves search relevance as they use model based embeddings. It works on how similar the documents are in the vector space 
relative to a search query.

We have implemented semantic search powered by model based embeddings to improve the search performance on engineering document repositories.
When the vanilla embeddings from the sentence-transformers (a flavor of BERT model fine-tuned for search) are used, the search performance is not optimal. 
So, we fine-tuned embeddings so that related documents are closer in the vector space whereas un-related documents are as far away as possible. 

We annotated a training set (few tens to hundreds of examples - query, related and unrelated pairs) and fine-tuned the model with 
the [**GPL**](https://github.com/UKPLab/gpl) technique. The training data creation can be automated as explained in this paper.

### 4. RLHF - Reinforcement Learning from Human Feedback












