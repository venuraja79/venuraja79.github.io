# Training Language Models for out of domain scenarios

## Overview
Language Models are the talk of the town. They produce state-of-the-art results in many tasks such as 
semantic search, text classification and summarization when compared with traditional NLP methods. 
However, when presented with same language tasks in a new domain (Engineering, Legal or Biomedical), 
they often come up short and perform less optimal than even traditional techniques. 

We are using a set of techniques to nudge the models to work well in the out of domain scenarios. All the out of domain
scenarios are not same! In some cases, the models can learn the nuances even with a few training examples. In others,
the language model itself have to be augmented. In this paper, we will discuss the techniques and how we leveraged them
in some of our projects to improve the model's performance.

## Language Models
Language models (BERT, GPT) just predict the next word given an initial prompt. They have been trained on large corpus of text 
and have gained intuitive understanding of the language structure, semantics and domain. We almost always download the 
pretrained model checkpoints from [**Hugging Face Hub**](https://huggingface.co) or other open hubs that host the models. 
We can't use these models as is and have to custom-train them. For example, a classification task
requires an architecture change (adding a classification head) and training with a classification dataset. This process is 
called fine-tuning.

## Customizing Language Models
Customizing a model can be as simple as training with a few examples on a particular language task. Sometimes, 
it can be a complex process that involves fully retraining the model with large corpus of domain specific text 
and augmenting its dictionary. We have experience in the following model customizations and this varies for different
problem statements.

* Fine-tuning on a custom task
* Augment the language model for a new domain
* Few-shot learning for semantic search
* Aligning the models towards human preferences with Reinforcement Learning

### 1. Task based fine-tuning
Many of the projects that we execute involve fine-tuning. We start with a task specific model checkpoint from the hub and
create a baseline on the task specific performance metric (accuracy, F1-score etc.,). For a Question Answering (QA) system, 
we used the SQUAD trained model as our baseline. This model generalizes well, if the customer's dataset is not very domain specific. 
We typically create a pipeline to generate annotated examples (question-answer pairs from user feedback) 
and fine-tune the model with this additional examples.

For text classification and NER, we start with a Language Model (LM) and add a classification head to it before fine-tuning with a
customer specific dataset as the labels are different for every problem. In the projects that we have executed, the time 
spent on collecting good training data provides the most bang for the buck than the architecture choices and hyperparameter tuning.

Some important points to consider -

* In many cases, a few hundred examples for each class can produce good enough models. When there is a change in architecture,
(addition of classification head) under-fitting is a real concern if the data sizes are small.

* Consider using smaller variants of the models such as DistillBERT to reduce both training and inference times without
losing too much accuracy.

* If the domain is entirely different, more training examples are required. We may also need to pretrain the LM itself, which is
explained in the next section.

* The Attention based transformer language models have a 512 token limitation. In most of the real-life projects, the documents
are large and doesn't fit this 512 token limit. We follow a combination of summarization & pooling methods to overcome this.

* Setup the MLOps process so that further training can be automated.

More details on the [training procedure] (https://huggingface.co/docs/transformers/training) is here.

### 2. Augment a Language model for a domain

When the problem domain is different, we first continue to pre-train the language model with domain specific text corpus. 
The augmented model is then fine-tuned as usual for specific tasks such as classification. This has produced better 
results ( a few % points gain in performance) for us in both legal and engineering domain problems that we have attempted.

Every generic pretrained LM comes with its own dictionary which is generated by a sub-word tokenization technique. The first 
step in the customization process is to preprocess the domain corpus to generate a domain specific token list (dictionary).
Then the embeddings for the new tokens are learnt by a training procedure using the domain text.

Please refer the [**ULMFit paper**](https://arxiv.org/pdf/1801.06146.pdf) and the [**notebook**](https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb) for more information.

It is also possible to create a domain specific LM from scratch without actually using any pretrained LM at all. But, this
process requires large text corpus and more GPU compute. This [**paper**](https://arxiv.org/abs/2007.15779) has more details
on this.

### 3. Few-shot learning for Semantic search

Semantic search is another focus area for us where we have used embeddings produced by LMs for implementing search on
engineering document repositories. 










